{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106d578",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e49d2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\Zetheta\\\\Project-23_Team1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41b1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['LANGSMITH_TRACING']=\"true\"\n",
    "os.environ['LANGSMITH_ENDPOINT']= os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ['LANGSMITH_API_KEY']= os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ['LANGSMITH_PROJECT']= \"AI_Chatbot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b12e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3545b77",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_url: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d95e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_ChatBot.constants import configFilePath,paramsFilePath\n",
    "from AI_ChatBot.utils.common import read_yaml,createDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path = configFilePath,params_path=paramsFilePath):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        createDir([self.config.artifacts_root])\n",
    "    \n",
    "    def get_DataIngestionConfig(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        createDir([config.root_dir])\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_url=config.source_url,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir\n",
    "        )\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b77be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "from AI_ChatBot.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self,config:DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def download_file(self)->str:\n",
    "        try:\n",
    "            os.makedirs(self.config.root_dir,exist_ok=True)\n",
    "            url = self.config.source_url\n",
    "            zip_location = self.config.local_data_file\n",
    "            logger.info(f'Downloading Data from {url} to the location {zip_location}')\n",
    "            id = url.split('/')[-1]\n",
    "            prefix = 'https://drive.google.com/uc?/export=download&id=' + id\n",
    "            gdown.download_folder(url=url,output=zip_location)\n",
    "            logger.info(f'Data Download Completed')\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_DataIngestionConfig()\n",
    "    data_ingestion = DataIngestion(data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5a04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8265bf8",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    text_dir: Path\n",
    "    video_dir: Path\n",
    "    source_dir: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LoaderConfig:\n",
    "    mode: str\n",
    "    strategy: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RecursiveConfig:\n",
    "    chunk_size: int\n",
    "    chunk_overlap: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SemanticConfig:\n",
    "    percentile: int\n",
    "    breakpoint_threshold_type: str\n",
    "    breakpoint_threshold_amount: int\n",
    "    buffer_size: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationParams:\n",
    "    loader: LoaderConfig\n",
    "    recursive: RecursiveConfig\n",
    "    semantic: SemanticConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_ChatBot.constants import configFilePath,paramsFilePath\n",
    "from AI_ChatBot.utils.common import read_yaml,createDir\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path = configFilePath,params_path=paramsFilePath):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        createDir([self.config.artifacts_root])\n",
    "    \n",
    "    def get_DataTransformationConfig(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        createDir([config.root_dir])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir= Path(config.root_dir),\n",
    "            text_dir= Path(config.text_dir),\n",
    "            video_dir= Path(config.video_dir),\n",
    "            source_dir= Path(config.source_dir)\n",
    "        )\n",
    "        return data_transformation_config\n",
    "\n",
    "    def getLoaderConfig(self)->LoaderConfig:\n",
    "        params = self.params.loader\n",
    "\n",
    "        return LoaderConfig(\n",
    "            mode=str(params.mode),\n",
    "            strategy=str(params.strategy)\n",
    "        )\n",
    "\n",
    "    def getRecursiveConfig(self)->RecursiveConfig:\n",
    "        params = self.params.recursive\n",
    "\n",
    "        return RecursiveConfig(\n",
    "            chunk_size=int(params.chunk_size),\n",
    "            chunk_overlap=int(params.chunk_overlap)\n",
    "        )\n",
    "\n",
    "    def getSemanticConfig(self)->SemanticConfig:\n",
    "        params = self.params.semantic\n",
    "\n",
    "        return SemanticConfig(\n",
    "            percentile = int(params.percentile),\n",
    "            breakpoint_threshold_type = str(params.breakpoint_threshold_type),\n",
    "            breakpoint_threshold_amount = int(params.breakpoint_threshold_amount),\n",
    "            buffer_size = int(params.buffer_size)\n",
    "        )\n",
    "\n",
    "    def getDataTransformationParams(self)->DataTransformationParams:\n",
    "\n",
    "        return DataTransformationParams(\n",
    "            loader = self.getLoaderConfig(),\n",
    "            recursive = self.getRecursiveConfig(),\n",
    "            semantic = self.getSemanticConfig()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import torch\n",
    "import shutil\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import unstructured\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from AI_ChatBot.logging import logger\n",
    "from AI_ChatBot.utils.common import createDir\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader,TextLoader,UnstructuredWordDocumentLoader\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self,config:DataTransformationConfig,params:DataTransformationParams,max_workers: int = None):\n",
    "      self.config = config\n",
    "      self.params = params\n",
    "      load_dotenv()\n",
    "      self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "      if self.api_key:\n",
    "        logger.info(\"Using OpenAI embeddings\")\n",
    "        self.embeddings = OpenAIEmbeddings(api_key=self.api_key,model=\"text-embedding-3-small\")\n",
    "      else:\n",
    "        logger.info(\"Using HuggingFace embeddings\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",model_kwargs={'device': device})\n",
    "\n",
    "      self.max_workers = max_workers or os.cpu_count() or 4\n",
    "      self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "          chunk_size=self.params.recursive.chunk_size,\n",
    "          chunk_overlap=self.params.recursive.chunk_overlap\n",
    "      )\n",
    "      self.chunker = SemanticChunker(\n",
    "              embeddings=self.embeddings,\n",
    "              breakpoint_threshold_type=self.params.semantic.breakpoint_threshold_type,\n",
    "              breakpoint_threshold_amount=self.params.semantic.breakpoint_threshold_amount,\n",
    "              buffer_size=self.params.semantic.buffer_size\n",
    "          )\n",
    "\n",
    "\n",
    "    def separate_files(self):\n",
    "      \"\"\"\n",
    "      Separate Files based on extensions into TEXT and VIDEO\n",
    "      \"\"\"\n",
    "      createDir([self.config.text_dir])\n",
    "      createDir([self.config.video_dir])\n",
    "      text_exts = {\"pdf\", \"docx\", \"doc\", \"txt\"}\n",
    "      logger.info(\"File Separation Started\")\n",
    "      path = Path(self.config.source_dir)\n",
    "      for src in path.iterdir():\n",
    "          if not src.is_file():\n",
    "              continue\n",
    "          dest_dir = self.config.text_dir if src.suffix.lower().lstrip('.') in text_exts else self.config.video_dir\n",
    "          dest = os.path.join(dest_dir,src.name)\n",
    "          if not os.path.exists(dest):\n",
    "              shutil.copy2(src, dest)\n",
    "      logger.info(\"File Separation Ended\")\n",
    "\n",
    "    def merge_titles_with_body(self,elements: List[Document]) -> List[Document]:\n",
    "      \"\"\"\n",
    "      Given a list of Document elements (from UnstructuredPDFLoader in mode=\"elements\"),\n",
    "      merge each Title element with its following body elements until the next Title.\n",
    "\n",
    "      Assumptions:\n",
    "      - elements are in document order.\n",
    "      - metadata['category'] indicates element type, e.g., 'Title', 'NarrativeText', 'UncategorizedText', etc.\n",
    "      - You may also consider page number: if metadata includes 'page_number', only merge within same page.\n",
    "      \"\"\"\n",
    "      merged: List[Document] = []\n",
    "      buffer_text = \"\"\n",
    "      buffer_meta = None\n",
    "\n",
    "      file_path = Path(elements[0].metadata.get(\"source\", \"\"))\n",
    "      with open(file_path,'rb') as f:\n",
    "            content_bytes = f.read()\n",
    "      content_hash = hashlib.md5(content_bytes).hexdigest()\n",
    "\n",
    "      def flush_buffer():\n",
    "          nonlocal buffer_text, buffer_meta\n",
    "          if buffer_text.strip() and buffer_meta is not None:\n",
    "              # Create a new Document with combined text and metadata from the heading or first element.\n",
    "              merged.append(Document(page_content=buffer_text.strip(), metadata=buffer_meta))\n",
    "          buffer_text = \"\"\n",
    "          buffer_meta = None\n",
    "      title_keywords = [\"title\", \"heading\", \"section_heading\", \"header\"]\n",
    "      for elem in elements:\n",
    "          meta = elem.metadata or {}\n",
    "          meta['content_hash'] = content_hash\n",
    "          meta['filename'] = file_path.name\n",
    "          category = meta.get(\"category\", \"\").lower()  # e.g., 'title', 'narrativetext', etc.\n",
    "          # Normalize category check:\n",
    "\n",
    "          is_title = any(key in category for key in title_keywords)\n",
    "\n",
    "          if is_title:\n",
    "              # If there's something buffered, flush it first\n",
    "              flush_buffer()\n",
    "              # Start a new buffer with the title text\n",
    "              buffer_text = elem.page_content or \"\"\n",
    "              # Keep metadata: you might want to preserve more fields; here we keep the title’s metadata,\n",
    "              # but you could attach combined metadata later (e.g., include page_number etc.)\n",
    "              new_meta = meta.copy()\n",
    "              # Track merged element IDs\n",
    "              eid = meta.get(\"element_id\")\n",
    "              if eid:\n",
    "                  new_meta[\"merged_from\"] = [eid]\n",
    "              buffer_meta = new_meta\n",
    "\n",
    "          else:\n",
    "              # A non-title element: either NarrativeText or UncategorizedText, etc.\n",
    "              if buffer_meta is not None:\n",
    "                  # Append body text under the last heading\n",
    "                  buffer_text += \"\\n\" + (elem.page_content or \"\")\n",
    "                  # Optionally track which element_ids are merged\n",
    "\n",
    "                  eid = elem.metadata.get(\"element_id\") if elem.metadata else None\n",
    "                  if eid:\n",
    "                      buffer_meta.setdefault(\"merged_from\", []).append(eid)\n",
    "              else:\n",
    "                  # No prior title: treat this as standalone block\n",
    "                  merged.append(elem)\n",
    "      # Flush final buffer\n",
    "      flush_buffer()\n",
    "      return merged\n",
    "\n",
    "    def _merge_small_chunks(self, chunks: List[Document], min_size: int) -> List[Document]:\n",
    "      \"\"\"\n",
    "      Merge adjacent small chunks until reaching at least min_size characters.\n",
    "      \"\"\"\n",
    "      merged: List[Document] = []\n",
    "      buffer_text = \"\"\n",
    "      buffer_meta = None\n",
    "\n",
    "      for doc in chunks:\n",
    "          text = doc.page_content or \"\"\n",
    "          if not text:\n",
    "              continue\n",
    "          if buffer_meta is None:\n",
    "              # start new buffer\n",
    "              buffer_text = text\n",
    "              buffer_meta = doc.metadata.copy() if doc.metadata else {}\n",
    "\n",
    "              # track merged IDs if exist\n",
    "              eid = doc.metadata.get(\"element_id\") if doc.metadata else None\n",
    "              if eid:\n",
    "                  buffer_meta[\"merged_from\"] = [eid]\n",
    "          else:\n",
    "              # if current buffer is smaller than min_size, append\n",
    "              if len(buffer_text) < min_size:\n",
    "                  buffer_text += \"\\n\" + text\n",
    "                  # merge metadata element_id\n",
    "\n",
    "                  eid = doc.metadata.get(\"element_id\") if doc.metadata else None\n",
    "                  if eid:\n",
    "                      buffer_meta.setdefault(\"merged_from\", []).append(eid)\n",
    "              else:\n",
    "                  # flush existing buffer\n",
    "                  merged.append(Document(page_content=buffer_text.strip(), metadata=buffer_meta))\n",
    "                  # start new buffer\n",
    "                  buffer_text = text\n",
    "                  buffer_meta = doc.metadata.copy() if doc.metadata else {}\n",
    "                  eid = doc.metadata.get(\"element_id\") if doc.metadata else None\n",
    "                  if eid:\n",
    "                      buffer_meta[\"merged_from\"] = [eid]\n",
    "      # Flush last buffer\n",
    "      if buffer_meta is not None and buffer_text:\n",
    "          merged.append(Document(page_content=buffer_text.strip(), metadata=buffer_meta))\n",
    "      return merged\n",
    "\n",
    "\n",
    "    def _load_file(self, file_path: Path) -> list[Document]:\n",
    "      \"\"\"\n",
    "      Given file path\n",
    "      Load corresponding document loader,\n",
    "      Return List[Document]\n",
    "      \"\"\"\n",
    "      ext = file_path.suffix.lower()\n",
    "      LOADER_MAP = {\n",
    "          '.pdf': lambda p: UnstructuredPDFLoader(str(p), mode=self.params.loader.mode,strategy=self.params.loader.strategy),\n",
    "          '.docx': lambda p: UnstructuredWordDocumentLoader(str(p),mode=self.params.loader.mode,strategy=self.params.loader.strategy),\n",
    "          '.doc': lambda p: UnstructuredWordDocumentLoader(str(p),mode=self.params.loader.mode,strategy=self.params.loader.strategy),\n",
    "          '.txt': lambda p: TextLoader(str(p), autodetect_encoding=True),\n",
    "      }\n",
    "\n",
    "      loader_fn = LOADER_MAP.get(ext)\n",
    "\n",
    "      if not loader_fn:\n",
    "          logger.warning(f\"Unsupported file type: {file_path}\")\n",
    "          return []\n",
    "\n",
    "      try:\n",
    "          loader = loader_fn(file_path)\n",
    "          docs = loader.load()\n",
    "          if isinstance(docs, Document):\n",
    "                docs = [docs]\n",
    "\n",
    "          if self.params.loader.mode == \"elements\":\n",
    "            try:\n",
    "                docs = self.merge_titles_with_body(docs)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Title-body merging failed for {file_path}: {e}\")\n",
    "                # fallback: keep original docs\n",
    "            # Ensure all are Document instances\n",
    "          final_docs: List[Document] = []\n",
    "          for d in docs:\n",
    "              if isinstance(d, Document):\n",
    "                  final_docs.append(d)\n",
    "              else:\n",
    "                  final_docs.append(Document(page_content=str(d), metadata={\"source\": str(file_path),'filename':Path(file_path).name}))\n",
    "          return final_docs\n",
    "      except Exception as e:\n",
    "          logger.exception(f\"Failed to load {file_path}\")\n",
    "          return []\n",
    "\n",
    "    def chunk_docs(self,docs: List[Document],fname) -> List[Document]:\n",
    "      final_chunks:List[Document] = []\n",
    "      logger.info(\"Chunking Started for file {fname}\")\n",
    "      global_idx = 0\n",
    "      for sec_doc in docs:\n",
    "        sec_text = sec_doc.page_content or \"\"\n",
    "        if not sec_text.strip():\n",
    "          continue\n",
    "        sub_docs = [sec_doc]\n",
    "        for sub_doc in sub_docs:\n",
    "          text = sub_doc.page_content or \"\"\n",
    "          if len(text) <= self.params.recursive.chunk_size:\n",
    "            chunk_meta = sub_doc.metadata.copy() if sub_doc.metadata else {}\n",
    "            idx = 0\n",
    "            content_hash = chunk_meta.get('content_hash',\"\")\n",
    "            heading = chunk_meta.get(\"category\",\"\").replace(\" \",\"_\")\n",
    "            chunk_id = f\"{content_hash}_{heading}_{global_idx}\"\n",
    "            chunk_meta['chunk_idx'] = global_idx\n",
    "            chunk_meta['chunk_id'] = chunk_id\n",
    "            final_chunks.append(Document(page_content=text,metadata=chunk_meta))\n",
    "            global_idx += 1\n",
    "          else:\n",
    "            split_docs = self.recursive_splitter.split_text(text)\n",
    "            for idx,piece in enumerate(split_docs):\n",
    "              piece = piece.strip()\n",
    "              if not piece:\n",
    "                continue\n",
    "              chunk_meta = sub_doc.metadata.copy() if sub_doc.metadata else {}\n",
    "              content_hash = chunk_meta.get('content_hash',\"\")\n",
    "              heading = chunk_meta.get(\"category\",\"\").replace(\" \",\"_\")\n",
    "              chunk_id = f\"{content_hash}_{heading}_{global_idx}\"\n",
    "              chunk_meta['chunk_idx'] = global_idx\n",
    "              chunk_meta['chunk_id'] = chunk_id\n",
    "              final_chunks.append(Document(page_content=piece,metadata=chunk_meta))\n",
    "              global_idx += 1\n",
    "      logger.info(f\"Chunking completed: {len(final_chunks)} chunks for file {fname}\")\n",
    "      return final_chunks\n",
    "\n",
    "\n",
    "    def load_documents(self):\n",
    "      self.separate_files()\n",
    "      logger.info(\"Document loading started\")\n",
    "\n",
    "      docs = []\n",
    "      chunks = []\n",
    "      files = list(Path(self.config.text_dir).iterdir())\n",
    "      # Parallelize document loading\n",
    "      with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "          for doc_list in executor.map(self._load_file, files):\n",
    "              docs.append(doc_list)\n",
    "      flat_docs = []\n",
    "      for doc in docs:\n",
    "        flat_docs.extend(doc)\n",
    "        fname = \"\"\n",
    "        if len(doc):\n",
    "          fname = Path(doc[0].metadata.get('source','')).name\n",
    "        chunks.extend(self.chunk_docs(doc,fname))\n",
    "      logger.info(f\"Document loading completed: {len(docs)} documents\")\n",
    "      return flat_docs,chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13803ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    data_transformation_config = configuration_manager.get_DataTransformationConfig()\n",
    "    data_transformation_params = configuration_manager.getDataTransformationParams()\n",
    "    data_transformation = DataTransformation(data_transformation_config,data_transformation_params)\n",
    "    docs,chunks = data_transformation.load_documents()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169bc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c97dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b5fb45",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VectorizationConfig:\n",
    "    batch_embed: int\n",
    "    batch_table: int\n",
    "    docs: str\n",
    "    chunks: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469de658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_ChatBot.constants import configFilePath,paramsFilePath\n",
    "from AI_ChatBot.utils.common import read_yaml,createDir\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path = configFilePath,params_path=paramsFilePath):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        createDir([self.config.artifacts_root])\n",
    "\n",
    "    def getVectorizationConfig(self)->VectorizationConfig:\n",
    "        params = self.params.supabase\n",
    "\n",
    "        return VectorizationConfig(\n",
    "            batch_embed=int(params.batch_embed),\n",
    "            batch_table=int(params.batch_table),\n",
    "            docs=str(params.docs),\n",
    "            chunks=str(params.chunks)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "from supabase import create_client, Client\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from AI_ChatBot.logging import logger\n",
    "\n",
    "class DataVectorization:\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialize the vectorization pipeline.\n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "\n",
    "        self.supabase_url =  os.getenv(\"SUPABASE_URL\") \n",
    "        self.supabase_key = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "        if not self.supabase_url or not self.supabase_key:\n",
    "            raise ValueError(\"Supabase credentials not found\")\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.api = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if self.api:\n",
    "            logger.info(\"Using OpenAI embeddings\")\n",
    "            self.model = OpenAIEmbeddings(api_key=self.api)\n",
    "        else:\n",
    "            logger.info('Using HuggingFaceEmbeddings')\n",
    "            self.model = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                model_kwargs={'device': device}\n",
    "            )\n",
    "\n",
    "        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)\n",
    "\n",
    "    def get_unique_fields(self,chunks: List[Document]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract a set of unique filenames from a list of document chunks.\n",
    "\n",
    "        Args:\n",
    "            chunks (List[Document]): List of Document objects.\n",
    "\n",
    "        Returns:\n",
    "            Set[str]: Set of unique filenames.\n",
    "        \"\"\"\n",
    "        return list(set(\n",
    "            chunk.metadata.get(\"filename\", \"unknown\") for chunk in chunks\n",
    "        ))\n",
    "\n",
    "    def upsert_document_details(self,chunks: List[Document]):\n",
    "        \"\"\"\n",
    "        Upsert document details in Supabase.\n",
    "        \"\"\"\n",
    "        unique_filenames = self.get_unique_fields(chunks)\n",
    "        mp = defaultdict(str)\n",
    "        unique_pairs = {\n",
    "            (\n",
    "                chunk.metadata.get(\"filename\", \"unknown\"),\n",
    "                chunk.metadata.get(\"content_hash\", \"unknown\")\n",
    "            )\n",
    "            for chunk in chunks\n",
    "        }\n",
    "        for fname,hash in unique_pairs:\n",
    "          mp[fname] = hash\n",
    "        doc_id = {}\n",
    "        for filename in unique_filenames:\n",
    "          resp = self.supabase.table(self.params.docs).select(\"id\",\"version\").eq(\"title\",filename).execute()\n",
    "          data = resp.data\n",
    "          if data:\n",
    "            doc = data[0]\n",
    "            document_id = doc[\"id\"]\n",
    "            new_version = doc[\"version\"] + 1\n",
    "            update_resp = self.supabase.table(self.params.docs).update({\n",
    "                \"version\": new_version,\n",
    "                \"updated_at\": datetime.now().isoformat()\n",
    "            }).eq(\"id\", document_id).execute()\n",
    "            doc_id[filename] = document_id\n",
    "          else:\n",
    "              # Insert new document row\n",
    "              ext = filename.split('.')[-1]\n",
    "              insert_resp = self.supabase.table(self.params.docs).insert({\n",
    "                  \"id\": mp[filename],\n",
    "                  \"title\": filename,\n",
    "                  \"file_type\": filename.split('.')[-1],\n",
    "                  \"version\": 1,\n",
    "                  \"updated_at\": datetime.now().isoformat()\n",
    "\n",
    "              }).execute()\n",
    "              new_doc = insert_resp.data[0]\n",
    "              doc_id[filename] = new_doc[\"id\"]\n",
    "        return doc_id\n",
    "\n",
    "    def batch_embed(self, texts: List[str], batch_size: int):\n",
    "        \"\"\"Create embeddings in batches to avoid memory issues.\"\"\"\n",
    "        logger.info(\"Batch embedding along with normalization has started\")\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeds = self.model.embed_documents(batch)\n",
    "            for emb in batch_embeds:\n",
    "              arr = np.array(emb, dtype=\"float32\")\n",
    "              norm = np.linalg.norm(arr)\n",
    "              if norm > 0:\n",
    "                  arr /= norm\n",
    "              all_embeddings.append(arr.tolist())\n",
    "        logger.info(\"Batch embedding along with normalization has completed\")\n",
    "        return all_embeddings\n",
    "    def _normalize(self, vec: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        L2-normalize a vector so cosine similarity can be done via Euclidean <-> if needed.\n",
    "        pgvector `<=>` operator computes cosine distance directly, so normalization is optional.\n",
    "        But normalizing ensures Euclidean `<->` works as cosine if pgvector version uses `<->`.\n",
    "        \"\"\"\n",
    "        arr = np.array(vec, dtype=\"float32\")\n",
    "        norm = np.linalg.norm(arr)\n",
    "        if norm > 0:\n",
    "            arr /= norm\n",
    "        return arr.tolist()\n",
    "    def ingest_chunks(self,chunks:List[Document]):\n",
    "        logger.info(\"Document details Upsertion Started\")\n",
    "        doc_ids = self.upsert_document_details(chunks)\n",
    "        logger.info(\"Document details upserted\")\n",
    "        records = []\n",
    "        text = [doc.page_content for doc in chunks]\n",
    "        embeddings = self.batch_embed(text, self.params.batch_embed)\n",
    "        logger.info(\"Records generation Started\")\n",
    "        for doc,emb in zip(chunks,embeddings):\n",
    "            meta_data = {\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                \"filename\": doc.metadata.get(\"filename\", \"unknown\"),\n",
    "                'languages': doc.metadata.get('languages', 'unknown'),\n",
    "                'category': doc.metadata.get('category', 'unknown'),\n",
    "                'content_hash': doc.metadata.get('content_hash', 'unknown'),\n",
    "                'page_number': doc.metadata.get('page_number')\n",
    "            }\n",
    "        #   if hasattr(self.model, \"embed_documents\"):\n",
    "        #       vec = self.model.embed_documents([doc.page_content])[0]\n",
    "        #   else:\n",
    "        #       vec = self.model.embed_query(doc.page_content)\n",
    "        #   emb = self._normalize(vec)\n",
    "            records.append({\n",
    "                    \"id\": doc.metadata.get(\"chunk_id\", \"unknown\"),\n",
    "                    \"document_id\": doc_ids[doc.metadata.get(\"filename\", \"unknown\")],\n",
    "                    \"chunk_index\": doc.metadata.get(\"chunk_idx\", \"unknown\"),\n",
    "                    \"text\": doc.page_content,\n",
    "                    \"embedding\": emb,\n",
    "                    \"metadata\": meta_data or {},\n",
    "                    \"updated_at\": datetime.now().isoformat()\n",
    "                })\n",
    "        logger.info(\"Records generation Ended\")\n",
    "        batch_size = self.params.batch_table\n",
    "        for i in range(0,len(records),batch_size):\n",
    "            batch = records[i:i+batch_size]\n",
    "            response = self.supabase.table(self.params.chunks).upsert(\n",
    "                batch,\n",
    "                on_conflict=\"id\"\n",
    "            ).execute()\n",
    "            logger.info(f\"Upserted chunks in the batch {i}–{i + len(batch)}\")\n",
    "            if not response.data:\n",
    "                raise Exception(f\"Supabase insert failed: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c030d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    data_vectorization_config = configuration_manager.getVectorizationConfig()\n",
    "    data_vectorization = DataVectorization(data_vectorization_config)\n",
    "    embed_status = data_vectorization.ingest_chunks(chunks)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88001c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b25dc2a",
   "metadata": {},
   "source": [
    "# query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17512f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class QueryConfig:\n",
    "    similarity_threshold: int\n",
    "    top_k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_ChatBot.constants import configFilePath,paramsFilePath\n",
    "from AI_ChatBot.utils.common import read_yaml,createDir\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path = configFilePath,params_path=paramsFilePath):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        createDir([self.config.artifacts_root])\n",
    "    \n",
    "    \n",
    "    def getQueryConfig(self)->QueryConfig:\n",
    "        params = self.params.query\n",
    "\n",
    "        return QueryConfig(\n",
    "            similarity_threshold=params.similarity_threshold,\n",
    "            top_k=params.top_k\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae95083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from AI_ChatBot.entity import QueryConfig\n",
    "from supabase import create_client, Client\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class QueryProcessor:\n",
    "    \"\"\"\n",
    "    Processes user queries: preprocessing, intent analysis, embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self,params:QueryConfig):\n",
    "        self.params = params\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        api = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if api:\n",
    "            self.embedding_model = OpenAIEmbeddings(api_key=api)\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                model_kwargs={'device': device}\n",
    "            )\n",
    "        self.supabase_url =  os.getenv(\"SUPABASE_URL\") \n",
    "        self.supabase_key = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "        if not self.supabase_url or not self.supabase_key:\n",
    "            raise ValueError(\"Supabase credentials not found\")\n",
    "\n",
    "        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)\n",
    "        self.table_name = \"chunks\"\n",
    "\n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        # Normalize whitespace\n",
    "        q = re.sub(r'\\s+', ' ', query.strip())\n",
    "        # Remove special chars except ?.,\n",
    "        q = re.sub(r'[^\\w\\s?.,-]', '', q)\n",
    "        # Ensure question mark if starts with interrogative\n",
    "        if re.match(r'^(?:what|when|where|who|why|how|is|are|can|could|would|should|do|does|did)\\b', q, re.I):\n",
    "              if not q.endswith(\"?\"):\n",
    "                  q += \"?\"\n",
    "        return q\n",
    "\n",
    "    def analyze_intent(self, query: str) -> Dict[str, Any]:\n",
    "        # Simple heuristic: detect question type\n",
    "        ql = query.lower()\n",
    "        if ql.startswith(\"what is\") or ql.startswith(\"define\"):\n",
    "            qtype = \"definition\"\n",
    "        elif \"compare\" in ql or \"vs\" in ql:\n",
    "            qtype = \"comparison\"\n",
    "        elif ql.startswith(\"how\") or ql.startswith(\"explain\"):\n",
    "            qtype = \"explanation\"\n",
    "        else:\n",
    "            qtype = \"general\"\n",
    "        # You may extend with keyword/entity extraction via spaCy or simple splitting\n",
    "        return {\"question_type\": qtype}\n",
    "\n",
    "    def _normalize(self, vec: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        L2-normalize a vector so cosine similarity can be done via Euclidean <-> if needed.\n",
    "        pgvector `<=>` operator computes cosine distance directly, so normalization is optional.\n",
    "        But normalizing ensures Euclidean `<->` works as cosine if pgvector version uses `<->`.\n",
    "        \"\"\"\n",
    "        arr = np.array(vec, dtype=\"float32\")\n",
    "        norm = np.linalg.norm(arr)\n",
    "        if norm > 0:\n",
    "            arr /= norm\n",
    "        return arr.tolist()\n",
    "    def search_similar_chunks(self,query:str)->List[Dict]:\n",
    "        if hasattr(self.embedding_model, \"embed_query\"):\n",
    "            vec = self.embedding_model.embed_query(query)\n",
    "        else:\n",
    "            # Some embedder only has embed_documents\n",
    "            vec = self.embedding_model.embed_documents([query])[0]\n",
    "        emb = self._normalize(vec)\n",
    "        try:\n",
    "          resp = self.supabase.rpc(\"match_chunks\",{\n",
    "              \"query_embedding\":emb,\n",
    "              \"match_count\":self.params.top_k\n",
    "          }).execute()\n",
    "        except Exception as e:\n",
    "          print(\"Supabase RPC error:\", e)\n",
    "          return []\n",
    "        data = getattr(resp,\"data\",None) or []\n",
    "        results = []\n",
    "        for row in data:\n",
    "          score = row.get(\"similarity\",0.0)\n",
    "          if score < self.params.similarity_threshold:\n",
    "            continue\n",
    "          results.append({\n",
    "              'score':score,\n",
    "              \"text\": row.get(\"text\",\"\"),\n",
    "              \"metadata\": row.get(\"metadata\",{})\n",
    "          })\n",
    "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return results\n",
    "\n",
    "    def process(self, raw_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Full pipeline up to retrieval. Returns intent, preprocessed query, and retrieved chunks.\n",
    "        \"\"\"\n",
    "        pre_q = self.preprocess_query(raw_query)\n",
    "        intent = self.analyze_intent(pre_q)\n",
    "        retrieved = self.search_similar_chunks(pre_q)\n",
    "        # Attach intent and preprocessed query to results\n",
    "        return {\n",
    "            \"preprocessed_query\": pre_q,\n",
    "            \"intent\": intent,\n",
    "            \"retrieved_chunks\": retrieved\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    query_config = configuration_manager.getQueryConfig()\n",
    "    query_processor = QueryProcessor(query_config)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    def __init__(self, system_message: str = None):\n",
    "        # system_message: a fixed instruction header\n",
    "        self.system_message = system_message or (\n",
    "            \"You are an expert educational assistant for financial markets, Fintech, and AI concepts. \"\n",
    "            \"Use the provided context from the organization's knowledge base to answer accurately. \"\n",
    "            \"If information is unavailable in the context, indicate that you cannot find it.\"\n",
    "        )\n",
    "\n",
    "    def build_prompt(self, user_query: str, context_chunks: List[Dict[str, Any]]) -> str:\n",
    "        parts = [f\"SYSTEM: {self.system_message}\", \"\", \"CONTEXT:\"]\n",
    "        for chunk in context_chunks:\n",
    "            title = chunk['metadata'].get(\"filename\", \"Unknown\")\n",
    "            meta = chunk.get(\"metadata\", {})\n",
    "            content = chunk.get(\"text\", \"\")\n",
    "            parts.append(f\"--- Source: {title}, metadata: {meta}\")\n",
    "            parts.append(content)\n",
    "            parts.append(\"---\")\n",
    "\n",
    "        parts.append(\"\")\n",
    "        parts.append(f\"USER QUESTION: {user_query}\")\n",
    "        parts.append(\"\")\n",
    "        parts.append(\n",
    "            \"INSTRUCTIONS: If the context does not contain the answer, state that the knowledge base does not cover it.\" \n",
    "            \"Cite sources in brackets when referencing facts. \"\n",
    "            \"Answer clearly and concisely based only on the provided context. \"\n",
    "        )\n",
    "        return \"\\n\".join(parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4572f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAJGNNcQnQu41N7ZGsjbqx8n3GXhgVFd10\"\n",
    "query = \"What is profit\"\n",
    "clean_query = query_processor.preprocess_query(query)\n",
    "res = query_processor.process(query)\n",
    "prompt_builder = PromptBuilder()\n",
    "refined_prompt = lambda inputs: prompt_builder.build_prompt(inputs['question'], inputs['retrieved_chunks'])\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "chain = refined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({'question': clean_query, 'retrieved_chunks': res['retrieved_chunks']})\n",
    "print(\"\\n Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['retrieved_chunks'][0]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import List, TypedDict\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Dict[str,Any]]\n",
    "    answer: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa14e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state:State):\n",
    "    clean_query = query_processor.preprocess_query(state['question'])\n",
    "    res = query_processor.process(state['question'])\n",
    "    return {\"context\": res['retrieved_chunks']}\n",
    "\n",
    "def generate(state:State):\n",
    "    prompt_builder = PromptBuilder()\n",
    "    refined_prompt = lambda inputs: prompt_builder.build_prompt(inputs['question'], inputs['retrieved_chunks'])\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "    chain = refined_prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({'question': state['question'], 'retrieved_chunks': state['context']})\n",
    "    return {'answer':response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "response = graph.invoke({\"question\": \"What is profit?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ccc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from AI_ChatBot.logging import logger\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    try:\n",
    "        res = query_processor.process(query)\n",
    "        docs = res['retrieved_chunks']\n",
    "        if not docs:\n",
    "            return \"\", []\n",
    "        serialized_parts = []\n",
    "        for doc in docs:\n",
    "            meta = doc['metadata'] or {}\n",
    "            source = meta.get(\"filename\",'unknown')\n",
    "            citation = f\"Source: {source}\"\n",
    "            content = doc['text']\n",
    "            serialized_parts.append(f\"{citation}\\nContent: {content}\")\n",
    "        serialized = \"\\n\\nContext:\\n\" + \"\\n\\n\".join(serialized_parts)\n",
    "        return serialized, docs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in retrieve tool: {e}\")\n",
    "        return \"\", []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import asyncio\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "async def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = await asyncio.get_event_loop().run_in_executor(None,lambda: llm_with_tools.invoke(state[\"messages\"]))\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "async def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_parts = [\n",
    "        \"You are an expert educational assistant for financial markets, Fintech, and AI concepts. Created by Zetheta.\",\n",
    "        \"Use the provided context from the organization's knowledge base to answer accurately.\",\n",
    "        \"If information is unavailable in the context, indicate that you cannot find it.\"\n",
    "        \"INSTRUCTIONS: \"\n",
    "            \"If the context does not contain the answer, state that the knowledge base does not cover it.\" \n",
    "            \"Cite sources in brackets when referencing facts. \"\n",
    "            \"Answer clearly and concisely based only on the provided context. \"\n",
    "    ]\n",
    "    if docs_content:\n",
    "        system_parts.append(\"\\n Context provided below:\")\n",
    "        system_parts.append(docs_content)\n",
    "    system_prompt = \"\\n\".join(system_parts)\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_prompt)] + conversation_messages\n",
    "    try:\n",
    "        response = await asyncio.get_event_loop().run_in_executor(None,lambda: llm.invoke(prompt))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate node: {e}\")\n",
    "        response = AIMessage(content=\"I'm sorry, I encountered an error generating the response.\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = FastAPI()\n",
    "\n",
    "# class ChatRequest(BaseModel):\n",
    "#     session_id: str = None\n",
    "#     message: str\n",
    "\n",
    "# class ChatResponse(BaseModel):\n",
    "#     session_id: str\n",
    "#     response: str\n",
    "\n",
    "# # Supabase table 'chat_history' schema: session_id (text), role (text), content (text), timestamp (timestamptz default now())\n",
    "# @app.post(\"/chat\", response_model=ChatResponse)\n",
    "# async def chat_endpoint(req: ChatRequest):\n",
    "#     session_id = req.session_id or str(uuid.uuid4())\n",
    "#     user_msg = req.message\n",
    "#     # Fetch prior history from Supabase\n",
    "#     history = []\n",
    "#     try:\n",
    "#         resp = supabase.table('chat_history').select('role, content').eq('session_id', session_id).order('timestamp', {'ascending': True}).execute()\n",
    "#         for rec in resp.data:\n",
    "#             history.append({\"role\": rec['role'], \"content\": rec['content']})\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching history: {e}\")\n",
    "#     # Append current user message\n",
    "#     history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "#     # Invoke graph with full history\n",
    "#     try:\n",
    "#         result = graph.invoke({\"messages\": history})\n",
    "#         assistant_msg = result[\"messages\"][-1].content\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in graph invocation: {e}\")\n",
    "#         raise HTTPException(status_code=500, detail=\"Internal error\")\n",
    "#     # Persist user and assistant messages to Supabase\n",
    "#     try:\n",
    "#         supabase.table('chat_history').insert([\n",
    "#             {\"session_id\": session_id, \"role\": \"user\", \"content\": user_msg},\n",
    "#             {\"session_id\": session_id, \"role\": \"assistant\", \"content\": assistant_msg}\n",
    "#         ]).execute()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving history: {e}\")\n",
    "#     return ChatResponse(session_id=session_id, response=assistant_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828390fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client\n",
    "supabase_url = \"https://girxqaleohymkbddljqf.supabase.co\"\n",
    "supabase_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdpcnhxYWxlb2h5bWtiZGRsanFmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDkxOTkwMzksImV4cCI6MjA2NDc3NTAzOX0.-wCqoiJcON7Dndr0N_EFigsMrCTr2SGWWx3S8Cr-56k\"\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "import uuid\n",
    "def chat_endpoint(query):\n",
    "    session_id = str(uuid.uuid4())\n",
    "    history = []\n",
    "    try:\n",
    "        resp = supabase.table('chat_history').select('role','content').eq('session_id',session_id).order('timestamp',{'ascending':True}).execute()\n",
    "        for rec in resp.data:\n",
    "            history.append({\"role\": rec['role'], \"content\": rec['content']})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching history: {e}\")\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "    try:\n",
    "        result = graph.invoke({'messages':history})\n",
    "        assistant_msg = result['messages'][-1].content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in graph invocation: {e}\")\n",
    "    try:\n",
    "        supabase.table('chat_history').insert([\n",
    "            {'session_id':session_id,'role':'user','content':query},\n",
    "            {'session_id':session_id,'role':'assistant','content':assistant_msg}\n",
    "        ]).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving history: {e}\")\n",
    "    return assistant_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = str(input(\"Enter query or exit\"))\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    print(chat_endpoint(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd229c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e452ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What is profit?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"what is the porters five analysis\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\"\n",
    ")[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"what is swot analysis\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\"\n",
    ")[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"what is the difference between these two\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\"\n",
    ")[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d0c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759ef0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc51c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68c31f7",
   "metadata": {},
   "source": [
    "# LLM and Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc0de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-22 14:17:00,265: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-22 14:17:00,278: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-06-22 14:17:00,283: INFO: common: Created director at: artifacts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Appsb\\Desktop\\Zetheta\\Project-23_Team1\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-22 14:17:17,516: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2]\n"
     ]
    }
   ],
   "source": [
    "from AI_ChatBot.pipeline.queryprocessing import QueryProcessingPipeline\n",
    "queryProcessing = QueryProcessingPipeline()\n",
    "query_processor = queryProcessing.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is profit\"\n",
    "clean_query = query_processor.preprocess_query(query)\n",
    "res = query_processor.process(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from AI_ChatBot.logging import logger\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    try:\n",
    "        res = query_processor.process(query)\n",
    "        docs = res['retrieved_chunks']\n",
    "        if not docs:\n",
    "            return \"\", []\n",
    "        serialized_parts = []\n",
    "        for doc in docs:\n",
    "            meta = doc['metadata'] or {}\n",
    "            source = meta.get(\"filename\",'unknown')\n",
    "            citation = f\"Source: {source}\"\n",
    "            content = doc['text']\n",
    "            serialized_parts.append(f\"{citation}\\nContent: {content}\")\n",
    "        serialized = \"\\n\\nContext:\\n\" + \"\\n\\n\".join(serialized_parts)\n",
    "        return serialized, docs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in retrieve tool: {e}\")\n",
    "        return \"\", []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response =  llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_parts = [\n",
    "        \"You are an expert educational assistant for financial markets, Fintech, and AI concepts. Created by Zetheta.\",\n",
    "        \"Use the provided context from the organization's knowledge base to answer accurately.\",\n",
    "        \"If information is unavailable in the context, indicate that you cannot find it.\"\n",
    "        \"INSTRUCTIONS: \"\n",
    "            \"If the context does not contain the answer, state that the knowledge base does not cover it.\" \n",
    "            \"Cite sources in brackets when referencing facts. \"\n",
    "            \"Answer clearly and concisely based only on the provided context. \"\n",
    "    ]\n",
    "    if docs_content:\n",
    "        system_parts.append(\"\\n Context provided below:\")\n",
    "        system_parts.append(docs_content)\n",
    "    system_prompt = \"\\n\".join(system_parts)\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_prompt)] + conversation_messages\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate node: {e}\")\n",
    "        response = AIMessage(content=\"I'm sorry, I encountered an error generating the response.\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b49e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client,Client\n",
    "from AI_ChatBot.logging import logger\n",
    "from AI_ChatBot.components.query_processing import QueryProcessor\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage,AIMessage\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self,processor:QueryProcessor):\n",
    "        load_dotenv()\n",
    "        try: \n",
    "            os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "            self.llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "        except Exception as e:\n",
    "            # fall back\n",
    "            raise ValueError(\"Fallback is not implemented\")\n",
    "        \n",
    "        @tool(response_format=\"content_and_artifact\")\n",
    "        def retrieve(query: str):\n",
    "            \"\"\"Retrieve information related to a query.\"\"\"\n",
    "            try:\n",
    "                res = processor.process(query)\n",
    "                docs = res['retrieved_chunks']\n",
    "                if not docs:\n",
    "                    return \"\", []\n",
    "                serialized_parts = []\n",
    "                for doc in docs:\n",
    "                    meta = doc['metadata'] or {}\n",
    "                    source = meta.get(\"filename\",'unknown')\n",
    "                    citation = f\"Source: {source}\"\n",
    "                    content = doc['text']\n",
    "                    serialized_parts.append(f\"{citation}\\nContent: {content}\")\n",
    "                serialized = \"\\n\\nContext:\\n\" + \"\\n\\n\".join(serialized_parts)\n",
    "                return serialized, docs\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in retrieve tool: {e}\")\n",
    "                return \"\", []\n",
    "        \n",
    "        self.retrieve = retrieve\n",
    "    \n",
    "    \n",
    "    \n",
    "    def query_or_respond(self,state: MessagesState):\n",
    "        \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "        \n",
    "        llm_with_tools = self.llm.bind_tools([self.retrieve])\n",
    "        response =  llm_with_tools.invoke(state[\"messages\"])\n",
    "            \n",
    "        # MessagesState appends messages to state instead of overwriting\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    def generate(self,state: MessagesState):\n",
    "        \"\"\"Generate answer.\"\"\"\n",
    "        # Get generated ToolMessages\n",
    "        recent_tool_messages = []\n",
    "        for message in reversed(state[\"messages\"]):\n",
    "            if message.type == \"tool\":\n",
    "                recent_tool_messages.append(message)\n",
    "            else:\n",
    "                break\n",
    "        tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "        # Format into prompt\n",
    "        docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "        system_parts = [\n",
    "            \"You are an expert educational assistant for financial markets, Fintech, and AI concepts. Created by Zetheta.\",\n",
    "            \"Use the provided context from the organization's knowledge base to answer accurately.\",\n",
    "            \"If information is unavailable in the context, indicate that you cannot find it.\"\n",
    "            \"INSTRUCTIONS: \"\n",
    "                \"If the context does not contain the answer, state that the knowledge base does not cover it.\" \n",
    "                \"Cite sources in brackets when referencing facts. \"\n",
    "                \"Answer clearly and concisely based only on the provided context. \"\n",
    "        ]\n",
    "        if docs_content:\n",
    "            system_parts.append(\"\\n Context provided below:\")\n",
    "            system_parts.append(docs_content)\n",
    "        system_prompt = \"\\n\".join(system_parts)\n",
    "        conversation_messages = [\n",
    "            message\n",
    "            for message in state[\"messages\"]\n",
    "            if message.type in (\"human\", \"system\")\n",
    "            or (message.type == \"ai\" and not message.tool_calls)\n",
    "        ]\n",
    "        prompt = [SystemMessage(system_prompt)] + conversation_messages\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generate node: {e}\")\n",
    "            response = AIMessage(content=\"I'm sorry, I encountered an error generating the response.\")\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.graph_builder = StateGraph(MessagesState)\n",
    "        tools_node = ToolNode([self.retrieve])\n",
    "        self.graph_builder.add_node(self.query_or_respond)\n",
    "        self.graph_builder.add_node(tools_node)\n",
    "        self.graph_builder.add_node(self.generate)\n",
    "\n",
    "        self.graph_builder.set_entry_point(\"query_or_respond\")\n",
    "        self.graph_builder.add_conditional_edges(\n",
    "            \"query_or_respond\",\n",
    "            tools_condition,\n",
    "            {END: END, tools_node.name: tools_node.name},\n",
    "        )\n",
    "        self.graph_builder.add_edge(tools_node.name, \"generate\")\n",
    "        self.graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "        graph = self.graph_builder.compile()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "534c9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(query_processor)\n",
    "graph = rag.build_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a341e753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "296ba980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client\n",
    "supabase_url = \"https://girxqaleohymkbddljqf.supabase.co\"\n",
    "supabase_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdpcnhxYWxlb2h5bWtiZGRsanFmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDkxOTkwMzksImV4cCI6MjA2NDc3NTAzOX0.-wCqoiJcON7Dndr0N_EFigsMrCTr2SGWWx3S8Cr-56k\"\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "import uuid\n",
    "def chat_endpoint(query,session_id):\n",
    "    \n",
    "    history = []\n",
    "    try:\n",
    "        resp = supabase.table('chat_history').select('role','content',).eq('session_id',session_id).order('timestamp',desc=False).execute()\n",
    "        for rec in resp.data:\n",
    "            history.append({\"role\": rec['role'], \"content\": rec['content']})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching history: {e}\")\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "    try:\n",
    "        result = graph.invoke({'messages':history})\n",
    "        assistant_msg = result['messages'][-1].content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in graph invocation: {e}\")\n",
    "    try:\n",
    "        supabase.table('chat_history').insert([\n",
    "            {'session_id':session_id,'role':'user','content':query},\n",
    "            {'session_id':session_id,'role':'assistant','content':assistant_msg}\n",
    "        ]).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving history: {e}\")\n",
    "    return assistant_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a728f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-22 14:22:14,374: INFO: _client: HTTP Request: GET https://girxqaleohymkbddljqf.supabase.co/rest/v1/chat_history?select=role%2Ccontent&session_id=eq.276a2618-73e0-471a-afbb-105e2688f723&order=timestamp.asc \"HTTP/2 200 OK\"]\n",
      "[2025-06-22 14:22:16,049: INFO: _client: HTTP Request: POST https://girxqaleohymkbddljqf.supabase.co/rest/v1/rpc/match_chunks \"HTTP/2 200 OK\"]\n",
      "[2025-06-22 14:22:18,416: INFO: _client: HTTP Request: POST https://girxqaleohymkbddljqf.supabase.co/rest/v1/chat_history?columns=%22content%22%2C%22role%22%2C%22session_id%22 \"HTTP/2 201 Created\"]\n",
      "SWOT analysis involves identifying a company's internal strengths and weaknesses, as well as external opportunities and threats [Mastering investment analysis in portfolio management ver 1.1.docx], [Valuations and Fundamental Analysis with AI ver 1.2.docx]. Strengths provide a competitive advantage, while weaknesses may hinder competitiveness. Opportunities are external factors for potential growth, and threats are external factors that may pose challenges [Mastering investment analysis in portfolio management ver 1.1.docx], [Valuations and Fundamental Analysis with AI ver 1.2.docx].\n",
      "[2025-06-22 14:22:22,497: INFO: _client: HTTP Request: GET https://girxqaleohymkbddljqf.supabase.co/rest/v1/chat_history?select=role%2Ccontent&session_id=eq.276a2618-73e0-471a-afbb-105e2688f723&order=timestamp.asc \"HTTP/2 200 OK\"]\n",
      "[2025-06-22 14:22:25,661: INFO: _client: HTTP Request: POST https://girxqaleohymkbddljqf.supabase.co/rest/v1/chat_history?columns=%22content%22%2C%22role%22%2C%22session_id%22 \"HTTP/2 201 Created\"]\n",
      "Porter's Five Forces analysis is a framework for analyzing the competitive intensity and attractiveness of an industry. It identifies five key forces that shape industry competition:\n",
      "\n",
      "1.  **Threat of New Entrants:** This force examines how easily new competitors can enter the industry. Factors like barriers to entry, economies of scale, and government regulations play a role.\n",
      "\n",
      "2.  **Bargaining Power of Suppliers:** This force assesses the power of suppliers to drive up input prices. It depends on the number of suppliers, the availability of substitutes, and the importance of the supplier's product to the industry.\n",
      "\n",
      "3.  **Bargaining Power of Buyers:** This force analyzes the power of customers to drive down prices. It depends on the number of buyers, the availability of substitutes, and the importance of the product to the buyer.\n",
      "\n",
      "4.  **Threat of Substitute Products or Services:** This force examines the availability of alternative products or services that customers can switch to.\n",
      "\n",
      "5.  **Rivalry Among Existing Competitors:** This force assesses the intensity of competition among existing firms in the industry. Factors like the number of competitors, industry growth rate, and product differentiation play a role.\n",
      "\n",
      "By analyzing these five forces, companies can gain a better understanding of the industry's competitive landscape and identify opportunities to improve their profitability.\n",
      "[2025-06-22 14:22:35,599: INFO: _client: HTTP Request: GET https://girxqaleohymkbddljqf.supabase.co/rest/v1/chat_history?select=role%2Ccontent&session_id=eq.276a2618-73e0-471a-afbb-105e2688f723&order=timestamp.asc \"HTTP/2 200 OK\"]\n",
      "[2025-06-22 14:22:36,721: INFO: _client: HTTP Request: POST https://girxqaleohymkbddljqf.supabase.co/rest/v1/chat_history?columns=%22content%22%2C%22role%22%2C%22session_id%22 \"HTTP/2 201 Created\"]\n",
      "Your last question was \"whats porters five analysis\".\n"
     ]
    }
   ],
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "while True:\n",
    "    query = str(input())\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    print(chat_endpoint(query,session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e48e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"whats swot analysis\"\n",
    "chat_endpoint(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"whats porters five analysis\"\n",
    "chat_endpoint(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"whats my last question\"\n",
    "chat_endpoint(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a804987",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514320b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CacheConfig:\n",
    "    json_dir: Path\n",
    "    query_dir: Path\n",
    "    vectors_max_size: int\n",
    "    response_ttl: int\n",
    "    json_max_bytes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ee09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_ChatBot.constants import configFilePath,paramsFilePath\n",
    "from AI_ChatBot.utils.common import read_yaml,createDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b56d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path = configFilePath,params_path=paramsFilePath):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        createDir([self.config.artifacts_root])\n",
    "    \n",
    "    def get_CacheConfig(self) -> CacheConfig:\n",
    "        config = self.config.cache\n",
    "        params = self.params.cache\n",
    "\n",
    "        Path(config.json_dir).mkdir(parents=True,exist_ok=True)\n",
    "        \n",
    "        return CacheConfig(\n",
    "            json_dir=config.json_dir,\n",
    "            query_dir=config.query_dir,\n",
    "            vectors_max_size=params.vectors_max_size,\n",
    "            response_ttl=params.response_ttl,\n",
    "            json_max_bytes=params.json_max_bytes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecb6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def _hash_str(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _now_ts() -> float:\n",
    "    return time()\n",
    "\n",
    "def _is_expired(created_ts: float, ttl: int) -> bool:\n",
    "    return (time() - created_ts) > ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8257f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import threading\n",
    "class VectorCache:\n",
    "    \"\"\"In‑memory LRU cache for query embeddings.\"\"\"\n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.cache: Dict[str, Tuple[List[float], float]] = {}  # key → (vec, last_access_ts)\n",
    "        self.order: List[str] = []\n",
    "\n",
    "    def get(self, query: str) -> Optional[List[float]]:\n",
    "        key = _hash_str(query)\n",
    "        with self.lock:\n",
    "            if key in self.cache:\n",
    "                vec, _ = self.cache[key]\n",
    "                # update recency\n",
    "                self.cache[key] = (vec, _now_ts())\n",
    "                self.order.remove(key)\n",
    "                self.order.insert(0, key)\n",
    "                return vec\n",
    "        return None\n",
    "\n",
    "    def set(self, query: str, vec: List[float]):\n",
    "        key = _hash_str(query)\n",
    "        with self.lock:\n",
    "            if key in self.cache:\n",
    "                # update existing\n",
    "                self.order.remove(key)\n",
    "            elif len(self.order) >= self.max_size:\n",
    "                # evict LRU\n",
    "                lru = self.order.pop()\n",
    "                del self.cache[lru]\n",
    "            # insert as most recent\n",
    "            self.order.insert(0, key)\n",
    "            self.cache[key] = (vec, _now_ts())\n",
    "\n",
    "    def clear(self):\n",
    "        with self.lock:\n",
    "            self.cache.clear()\n",
    "            self.order.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a0d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "class QueryCache:\n",
    "    \"\"\"Persistent SQLite cache for raw query → metadata (e.g. stats, last used).\"\"\"\n",
    "    def __init__(self, db_path: Path):\n",
    "        self.db_path = db_path\n",
    "        self.conn = sqlite3.connect(str(db_path), check_same_thread=False)\n",
    "        self._init_table()\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _init_table(self):\n",
    "        self.conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS query_cache (\n",
    "            query_hash TEXT PRIMARY KEY,\n",
    "            query_text TEXT,\n",
    "            last_used TIMESTAMP,\n",
    "            use_count INTEGER DEFAULT 0\n",
    "        )\"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def touch(self, query: str):\n",
    "        \"\"\"Increment use_count & update last_used.\"\"\"\n",
    "        key = _hash_str(query)\n",
    "        now = datetime.now().isoformat()\n",
    "        with self.lock:\n",
    "            cur = self.conn.execute(\n",
    "                \"SELECT use_count FROM query_cache WHERE query_hash = ?\", (key,)\n",
    "            )\n",
    "            row = cur.fetchone()\n",
    "            if row:\n",
    "                count = row[0] + 1\n",
    "                self.conn.execute(\n",
    "                    \"UPDATE query_cache SET use_count = ?, last_used = ? WHERE query_hash = ?\",\n",
    "                    (count, now, key)\n",
    "                )\n",
    "            else:\n",
    "                self.conn.execute(\n",
    "                    \"INSERT INTO query_cache (query_hash, query_text, last_used, use_count) VALUES (?, ?, ?, 1)\",\n",
    "                    (key, query, now)\n",
    "                )\n",
    "            self.conn.commit()\n",
    "\n",
    "    def get_stats(self, query: str) -> Optional[Dict[str, Any]]:\n",
    "        key = _hash_str(query)\n",
    "        cur = self.conn.execute(\n",
    "            \"SELECT query_text, last_used, use_count FROM query_cache WHERE query_hash = ?\", (key,)\n",
    "        )\n",
    "        row = cur.fetchone()\n",
    "        if not row:\n",
    "            return None\n",
    "        return {\"query_text\": row[0], \"last_used\": row[1], \"use_count\": row[2]}\n",
    "\n",
    "    def prune_lru(self, max_entries: int):\n",
    "        \"\"\"Evict oldest entries beyond `max_entries`.\"\"\"\n",
    "        with self.lock:\n",
    "            cur = self.conn.execute(\n",
    "                \"SELECT query_hash FROM query_cache ORDER BY last_used ASC LIMIT -1 OFFSET ?;\",\n",
    "                (max_entries,)\n",
    "            )\n",
    "            keys = [r[0] for r in cur.fetchall()]\n",
    "            for k in keys:\n",
    "                self.conn.execute(\"DELETE FROM query_cache WHERE query_hash = ?\", (k,))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def clear(self):\n",
    "        with self.lock:\n",
    "            self.conn.execute(\"DELETE FROM query_cache\")\n",
    "            self.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b4d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "class ResponseCache:\n",
    "    \"\"\"\n",
    "    JSON‑file cache mapping (query + history hash) → response payload,\n",
    "    with TTL, LRU eviction, size limit, manual invalidation.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cache_dir: Path ,\n",
    "                 ttl_seconds: int,\n",
    "                 max_total_bytes: int ):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl = ttl_seconds\n",
    "        self.max_bytes = max_total_bytes\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _cache_path(self, key: str) -> Path:\n",
    "        return self.cache_dir / f\"{key}.json.gz\"\n",
    "\n",
    "    def _enforce_size_limit(self):\n",
    "        \"\"\"Evict LRU files if total size > max_bytes.\"\"\"\n",
    "        files = list(self.cache_dir.glob(\"*.json.gz\"))\n",
    "        files.sort(key=lambda p: p.stat().st_atime)  # oldest access first\n",
    "        total = sum(p.stat().st_size for p in files)\n",
    "        for p in files:\n",
    "            if total <= self.max_bytes:\n",
    "                break\n",
    "            size = p.stat().st_size\n",
    "            p.unlink()\n",
    "            total -= size\n",
    "\n",
    "    def get(self, query: str, history: List[Tuple[str, str]]) -> Optional[Dict[str, Any]]:\n",
    "        hist_serial = \"|\".join(f\"{u}→{a}\" for u, a in history)\n",
    "        key = _hash_str(query + \"||\" + hist_serial)\n",
    "        path = self._cache_path(key)\n",
    "        if not path.exists():\n",
    "            return None\n",
    "\n",
    "        with self.lock:\n",
    "            with gzip.open(path, \"rt\", encoding=\"utf-8\") as fp:\n",
    "                payload = json.load(fp)\n",
    "            created = payload.get(\"_created_at\", 0)\n",
    "            if _is_expired(created, self.ttl):\n",
    "                path.unlink()\n",
    "                return None\n",
    "            # update access time\n",
    "            os.utime(path, None)\n",
    "            return payload\n",
    "\n",
    "    def set(self,\n",
    "            query: str,\n",
    "            history: List[Tuple[str, str]],\n",
    "            response_payload: Dict[str, Any]):\n",
    "        hist_serial = \"|\".join(f\"{u}→{a}\" for u, a in history)\n",
    "        key = _hash_str(query + \"||\" + hist_serial)\n",
    "        path = self._cache_path(key)\n",
    "        payload = dict(response_payload)\n",
    "        payload[\"_created_at\"] = _now_ts()\n",
    "        with self.lock:\n",
    "            with gzip.open(path, \"wt\", encoding=\"utf-8\") as fp:\n",
    "                json.dump(payload, fp)\n",
    "            self._enforce_size_limit()\n",
    "\n",
    "    def invalidate(self, query: Optional[str] = None, history: Optional[List[Tuple[str, str]]] = None):\n",
    "        \"\"\"\n",
    "        - If no args: clear full response cache.\n",
    "        - If query only: delete all files whose key starts with hash(query).\n",
    "        - If both: delete the exact (query,history) entry.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if query is None:\n",
    "                # full wipe\n",
    "                for p in self.cache_dir.glob(\"*.json.gz\"):\n",
    "                    p.unlink()\n",
    "                return\n",
    "            # specific\n",
    "            hist_serial = \"\" if history is None else \"|\".join(f\"{u}→{a}\" for u, a in history)\n",
    "            pattern = _hash_str(query + \"||\" + hist_serial)\n",
    "            for p in self.cache_dir.glob(f\"{pattern}*.json.gz\"):\n",
    "                p.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481b865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    \"\"\"\n",
    "    Combines VectorCache, QueryCache, and ResponseCache into one interface.\n",
    "    \"\"\"\n",
    "    def __init__(self,config:CacheConfig):\n",
    "        self.config = config\n",
    "        self.vector_cache = VectorCache(max_size=self.config.vectors_max_size)\n",
    "        self.query_cache = QueryCache(db_path=self.config.query_dir)\n",
    "        self.response_cache = ResponseCache(\n",
    "            cache_dir=self.config.json_dir,\n",
    "            ttl_seconds=self.config.response_ttl,\n",
    "            max_total_bytes=self.config.json_max_bytes\n",
    "            )\n",
    "\n",
    "    # — Vector caching —\n",
    "    def get_vector(self, query: str) -> Optional[List[float]]:\n",
    "        return self.vector_cache.get(query)\n",
    "\n",
    "    def set_vector(self, query: str, vec: List[float]):\n",
    "        return self.vector_cache.set(query, vec)\n",
    "\n",
    "    # — Query metadata caching —\n",
    "    def touch_query(self, query: str):\n",
    "        return self.query_cache.touch(query)\n",
    "\n",
    "    def get_query_stats(self, query: str):\n",
    "        return self.query_cache.get_stats(query)\n",
    "\n",
    "    # — Response caching —\n",
    "    def get_response(self, query: str, history: List[Tuple[str, str]]) -> Optional[Dict[str, Any]]:\n",
    "        return self.response_cache.get(query, history)\n",
    "\n",
    "    def set_response(self,\n",
    "                     query: str,\n",
    "                     history: List[Tuple[str, str]],\n",
    "                     response_text: str,\n",
    "                     source_chunks: List[Dict[str, Any]],\n",
    "                     model: str,\n",
    "                     is_fallback: bool,\n",
    "                     latency: float):\n",
    "        payload = {\n",
    "            \"response_text\": response_text,\n",
    "            \"source_chunks\": source_chunks,\n",
    "            \"model\": model,\n",
    "            \"is_fallback\": is_fallback,\n",
    "            \"latency\": latency\n",
    "        }\n",
    "        return self.response_cache.set(query, history, payload)\n",
    "\n",
    "    def invalidate_response(self, query: Optional[str] = None, history: Optional[List[Tuple[str, str]]] = None):\n",
    "        return self.response_cache.invalidate(query, history)\n",
    "\n",
    "    # — Maintenance tasks —\n",
    "    def prune_query_cache(self, max_entries: int):\n",
    "        return self.query_cache.prune_lru(max_entries)\n",
    "\n",
    "    def clear_all(self):\n",
    "        self.vector_cache.clear()\n",
    "        self.query_cache.clear()\n",
    "        self.response_cache.invalidate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d58add",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_manager = ConfigurationManager()\n",
    "cache_config = configuration_manager.get_CacheConfig()\n",
    "cache = CacheManager(cache_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
